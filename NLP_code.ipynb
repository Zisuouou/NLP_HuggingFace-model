{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93df1f4b-27c4-49b6-b3c9-4eeaaf32c857",
   "metadata": {},
   "source": [
    "# Tasks 1. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7945814-f78a-4f28-9568-1436084e8818",
   "metadata": {},
   "source": [
    "- Implement a function called perform_ner(text) that takes a string text as input and performs Named Entity Recognition (NER) using a pre-trained model from Hugging Face.\n",
    "- The function should return a list of tuples, where each tuple contains a named entity and its corresponding entity type.\n",
    "- Write another function called display_entities(entities) that takes the list of tuples returned by perform_ner(text) and displays the named entities along with their entity types in a readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36aa0227-890c-48a1-af55-d672a3911515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large-finetuned-conll03-english were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities:\n",
      "엔터티: ▁Ji, 유형: I-PER\n",
      "엔터티: soo, 유형: I-PER\n",
      "엔터티: ▁Seoul, 유형: I-LOC\n",
      "엔터티: ▁South, 유형: I-LOC\n",
      "엔터티: ▁Korea, 유형: I-LOC\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# 모델 및 토크나이저 로드\n",
    "MODEL_NAME = \"xlm-roberta-large-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Named Entity Recognition (NER) 함수\n",
    "def perform_ner(text):\n",
    "    \"\"\"\n",
    "    Hugging Face 모델을 사용해 텍스트에서 Named Entity를 추출합니다.\n",
    "    :param text: 분석할 입력 텍스트 (string)\n",
    "    :return: (entity, type) 튜플의 리스트\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 텍스트를 토큰화\n",
    "        tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, is_split_into_words=False)\n",
    "        input_ids = tokens[\"input_ids\"]\n",
    "        attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "        # 모델 예측 수행\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        # 토큰 ID를 실제 단어와 매핑\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "        labels = predictions[0].tolist()\n",
    "\n",
    "        # 모델의 레이블 맵핑 가져오기\n",
    "        label_map = model.config.id2label\n",
    "        results = []\n",
    "\n",
    "        for token, label_id in zip(tokens, labels):\n",
    "            label = label_map[label_id]\n",
    "            if label != \"O\":  # \"O\"는 엔터티가 아님\n",
    "                results.append((token, label))\n",
    "\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error during NER processing: {e}\")\n",
    "        return []\n",
    "\n",
    "# Named Entity 결과 출력 함수\n",
    "def display_entities(entities):\n",
    "    \"\"\"\n",
    "    Named Entity Recognition 결과를 사람이 읽을 수 있는 형태로 출력합니다.\n",
    "    :param entities: NER 결과로 생성된 (entity, type) 튜플 리스트\n",
    "    \"\"\"\n",
    "    if not entities:\n",
    "        print(\"엔터티를 찾을 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    print(\"Named Entities:\")\n",
    "    for entity, entity_type in entities:\n",
    "        print(f\"엔터티: {entity}, 유형: {entity_type}\")\n",
    "\n",
    "# 사용 예제\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"Jisoo was born in Seoul, the capital of South Korea. 지수는 27살이다.\"\n",
    "    ner_results = perform_ner(sample_text)\n",
    "    display_entities(ner_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66860704-c30f-49d7-a4a8-aa46eeec259c",
   "metadata": {},
   "source": [
    "# Tasks 2. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a096ad1-038f-4f90-bb44-3eff0833c410",
   "metadata": {},
   "source": [
    "- Implement a function called perform_sentiment_analysis(text) that takes a string text as input and performs sentiment analysis using a pre-trained model from Hugging Face.\n",
    "- The function should return a dictionary containing the top 3 detected emotions and their corresponding scores.\n",
    "- Write another function called get_top_emotions(sentiment_dict) that takes the dictionary returned by perform_sentiment_analysis(text) and returns a list of the top 3 emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d32d2350-0a39-4659-b01d-b398fd940618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input Text: I will be happy working with BEN!\n",
      "Top 3 Emotions:\n",
      "Label: joy, Score: 0.9877\n",
      "Label: surprise, Score: 0.0051\n",
      "Label: sadness, Score: 0.0026\n",
      "\n",
      "Input Text: I'm feeling very excited and joyful!\n",
      "Top 3 Emotions:\n",
      "Label: joy, Score: 0.9933\n",
      "Label: surprise, Score: 0.0024\n",
      "Label: anger, Score: 0.0014\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model_name = \"j-hartmann/emotion-english-distilroberta-base\"  # 감정 분석 모델\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 감정 분석 함수\n",
    "def perform_sentiment_analysis(text):\n",
    "    \"\"\"\n",
    "    텍스트에서 감정 분석을 수행하고 상위 3개의 감정과 점수를 반환합니다.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 입력 텍스트 토큰화\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        \n",
    "        # 긴 텍스트 잘림 확인\n",
    "        if tokenizer.model_max_length and len(text) > tokenizer.model_max_length:\n",
    "            print(f\"Warning: Input text is longer than {tokenizer.model_max_length} tokens. It has been truncated.\")\n",
    "\n",
    "        # 모델 예측 수행\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # 소프트맥스 확률 계산\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # 상위 3개 감정 추출\n",
    "        topk = torch.topk(probabilities, k=3, dim=-1)\n",
    "        top_indices = topk.indices[0].tolist()\n",
    "        top_scores = topk.values[0].tolist()\n",
    "\n",
    "        # 레이블 맵핑\n",
    "        labels = model.config.id2label  # 모델에 정의된 감정 레이블 가져오기\n",
    "        top_labels = [labels[idx] for idx in top_indices]\n",
    "\n",
    "        # 결과 반환\n",
    "        return [{\"label\": top_labels[i], \"score\": top_scores[i]} for i in range(3)]\n",
    "    except Exception as e:\n",
    "        print(f\"Error during sentiment analysis: {e}\")\n",
    "        return []\n",
    "\n",
    "# 테스트 실행\n",
    "if __name__ == \"__main__\":\n",
    "    # 테스트 \n",
    "    test_texts = [\n",
    "        \"I will be happy working with BEN!\",\n",
    "        \"I'm feeling very excited and joyful!\",\n",
    "    ]\n",
    "    \n",
    "    for text in test_texts:\n",
    "        print(f\"\\nInput Text: {text}\")\n",
    "        results = perform_sentiment_analysis(text)\n",
    "        print(\"Top 3 Emotions:\")\n",
    "        for res in results:\n",
    "            print(f\"Label: {res['label']}, Score: {res['score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d304e4f4-3818-4c87-9bc9-68269d2f66e7",
   "metadata": {},
   "source": [
    "# Tasks 3. API Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce62c0c4-8384-436b-95d1-7145e0d267cf",
   "metadata": {},
   "source": [
    "- Create a simple API using FastAPI that accepts a text input and returns the named entities along with their corresponding entity types and the detected emotions.\n",
    "- Implement proper input validation and error handling in the API.\n",
    "    - If the text field is missing or empty in the request body, return a 400 Bad Request response with an appropriate error message.\n",
    "    - If an internal server error occurs during the processing of the request, return a 500 Internal Server Error response with an \n",
    "appropriate error message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46776073-e412-4257-a70d-32046f394b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b186320c-1678-4335-9968-cc34dde3ad91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large-finetuned-conll03-english were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# FastAPI 인스턴스 생성\n",
    "app = FastAPI()\n",
    "\n",
    "# NER 모델 및 토크나이저 로드\n",
    "NER_MODEL_NAME = \"xlm-roberta-large-finetuned-conll03-english\"\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained(NER_MODEL_NAME)\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(NER_MODEL_NAME)\n",
    "\n",
    "# Sentiment Analysis 모델 및 토크나이저 로드\n",
    "SENTIMENT_MODEL_NAME = \"j-hartmann/emotion-english-distilroberta-base\"\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(SENTIMENT_MODEL_NAME)\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(SENTIMENT_MODEL_NAME)\n",
    "\n",
    "# 요청 데이터 형식 정의\n",
    "class AnalyzeRequest(BaseModel):\n",
    "    text: str\n",
    "\n",
    "# NER 함수\n",
    "def perform_ner(text):\n",
    "    \"\"\"\n",
    "    텍스트에서 Named Entity를 추출합니다.\n",
    "    \"\"\"\n",
    "    tokens = ner_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = ner_model(**tokens)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    labels = predictions[0].tolist()\n",
    "    tokens = ner_tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])\n",
    "    label_map = ner_model.config.id2label\n",
    "\n",
    "    results = []\n",
    "    for token, label_id in zip(tokens, labels):\n",
    "        label = label_map[label_id]\n",
    "        if label != \"O\":  # \"O\"는 엔터티가 아님\n",
    "            results.append({\"entity\": token, \"type\": label})\n",
    "    return results\n",
    "\n",
    "# Sentiment Analysis 함수\n",
    "def perform_sentiment_analysis(text):\n",
    "    \"\"\"\n",
    "    텍스트에서 감정 분석을 수행하고 상위 3개의 감정을 반환합니다.\n",
    "    \"\"\"\n",
    "    inputs = sentiment_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = sentiment_model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    topk = torch.topk(probabilities, k=3, dim=-1)\n",
    "    top_indices = topk.indices[0].tolist()\n",
    "    top_scores = topk.values[0].tolist()\n",
    "\n",
    "    labels = sentiment_model.config.id2label\n",
    "    top_labels = [labels[idx] for idx in top_indices]\n",
    "\n",
    "    return [{\"label\": top_labels[i], \"score\": top_scores[i]} for i in range(3)]\n",
    "\n",
    "# API 엔드포인트 정의\n",
    "@app.post(\"/analyze\")\n",
    "async def analyze_text(request: AnalyzeRequest):\n",
    "    if not request.text.strip():\n",
    "        raise HTTPException(status_code=400, detail=\"The 'text' field cannot be empty.\")\n",
    "\n",
    "    try:\n",
    "        # Named Entity Recognition 수행\n",
    "        entities = perform_ner(request.text)\n",
    "\n",
    "        # Sentiment Analysis 수행\n",
    "        sentiments = perform_sentiment_analysis(request.text)\n",
    "\n",
    "        # 결과 반환\n",
    "        return {\n",
    "            \"entities\": entities,\n",
    "            \"emotions\": [sentiment[\"label\"] for sentiment in sentiments],\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Internal server error: {str(e)}\")\n",
    "\n",
    "# uvicorn script_name:app --reload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61741eba-276a-4a45-8ddd-fe86ee573990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
